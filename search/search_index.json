{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Welcome to the documentation for my k8s-gitops repo. This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a GitHub issue or join the k8s@home Discord if you have any questions. Thanks \u00b6 A lot of inspiration for my cluster came from the people that have shared their clusters over at the awesome-home-kubernetes repository.","title":"Welcome"},{"location":"#welcome","text":"Welcome to the documentation for my k8s-gitops repo. This repository is my home Kubernetes cluster in a declarative state. Flux watches my cluster folder and makes the changes to my cluster based on the YAML manifests. Feel free to open a GitHub issue or join the k8s@home Discord if you have any questions.","title":"Welcome"},{"location":"#thanks","text":"A lot of inspiration for my cluster came from the people that have shared their clusters over at the awesome-home-kubernetes repository.","title":"Thanks"},{"location":"gitops/","text":"Storage \u00b6","title":"Storage"},{"location":"gitops/#storage","text":"","title":"Storage"},{"location":"home/cluster_overview/","text":"Cluster overview \u00b6 My cluster is k3s provisioned on Ubuntu 21.04 nodes using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. See my ansible directory for my playbooks and roles. Hardware \u00b6 Device Count OS Disk Size Data Disk Size Ram Purpose Intel NUC8i3BEH 1 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Intel NUC8i5BEH 2 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Lenovo ThinkCentre M93p Tiny 1 256GB SSD N/A 8GB k3s Workers Synology DS918+ 1 N/A 3x6TB + 1x10TB SHR 16GB Shared file storage Kubernetes cluster components \u00b6 GitOps \u00b6 flux : Keeps the cluster in sync with this Git repository. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository. Networking \u00b6 See here for more information about my storage setup. calico : For internal cluster networking. kube-vip : Uses BGP to load balance the control-plane API, making it highly availible without requiring external HA proxy solutions. metallb : Uses layer 2 to provide Load Balancing features to my cluster. external-dns : Creates DNS entries in a separate CoreDNS deployment which lives on my router. Multus : For allowing pods to have multiple network interfaces. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt . Storage \u00b6 See here for more information about my storage setup. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage or CephFS storage. Kasten K10 : Data backup and recovery","title":"Cluster overview"},{"location":"home/cluster_overview/#cluster-overview","text":"My cluster is k3s provisioned on Ubuntu 21.04 nodes using the Ansible galaxy role ansible-role-k3s . This is a semi hyper-converged cluster, workloads and block storage are sharing the same available resources on my nodes. See my ansible directory for my playbooks and roles.","title":"Cluster overview"},{"location":"home/cluster_overview/#hardware","text":"Device Count OS Disk Size Data Disk Size Ram Purpose Intel NUC8i3BEH 1 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Intel NUC8i5BEH 2 512GB SSD 1TB NVMe (rook-ceph) 32GB k3s Masters (embedded etcd) Lenovo ThinkCentre M93p Tiny 1 256GB SSD N/A 8GB k3s Workers Synology DS918+ 1 N/A 3x6TB + 1x10TB SHR 16GB Shared file storage","title":"Hardware"},{"location":"home/cluster_overview/#kubernetes-cluster-components","text":"","title":"Kubernetes cluster components"},{"location":"home/cluster_overview/#gitops","text":"flux : Keeps the cluster in sync with this Git repository. Mozilla SOPS : Encrypts secrets which is safe to store - even to a public repository.","title":"GitOps"},{"location":"home/cluster_overview/#networking","text":"See here for more information about my storage setup. calico : For internal cluster networking. kube-vip : Uses BGP to load balance the control-plane API, making it highly availible without requiring external HA proxy solutions. metallb : Uses layer 2 to provide Load Balancing features to my cluster. external-dns : Creates DNS entries in a separate CoreDNS deployment which lives on my router. Multus : For allowing pods to have multiple network interfaces. cert-manager : Configured to create TLS certs for all ingress services automatically using LetsEncrypt .","title":"Networking"},{"location":"home/cluster_overview/#storage","text":"See here for more information about my storage setup. rook-ceph : Provides persistent volumes, allowing any application to consume RBD block storage or CephFS storage. Kasten K10 : Data backup and recovery","title":"Storage"},{"location":"home/repo_structure/","text":"Repository structure \u00b6 The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in my cluster before anything else exists core directory (depends on crds ) are important infrastructure applications that should never be pruned by Flux apps directory (depends on core ) is where my common applications (grouped by namespace) are placed. Flux will prune resources here if they are not tracked by Git anymore.","title":"Repository structure"},{"location":"home/repo_structure/#repository-structure","text":"The Git repository contains the following directories under cluster and are ordered below by how Flux will apply them. ./cluster \u251c\u2500\u2500 ./apps \u251c\u2500\u2500 ./base \u251c\u2500\u2500 ./core \u2514\u2500\u2500 ./crds base directory is the entrypoint to Flux crds directory contains custom resource definitions (CRDs) that need to exist globally in my cluster before anything else exists core directory (depends on crds ) are important infrastructure applications that should never be pruned by Flux apps directory (depends on core ) is where my common applications (grouped by namespace) are placed. Flux will prune resources here if they are not tracked by Git anymore.","title":"Repository structure"},{"location":"home/tools/","text":"Tools \u00b6 Tool Purpose direnv Sets environment variables and tool environments based on present working directory pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes Repository automation \u00b6 Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Tools"},{"location":"home/tools/#tools","text":"Tool Purpose direnv Sets environment variables and tool environments based on present working directory pre-commit Enforce code consistency and verifies no secrets are pushed stern Tail logs in Kubernetes","title":"Tools"},{"location":"home/tools/#repository-automation","text":"Github Actions for checking code formatting Rancher System Upgrade Controller to apply updates to k3s Renovate with the help of the k8s-at-home/renovate-helm-releases Github action keeps my application charts and container images up-to-date","title":"Repository automation"},{"location":"networking/","text":"Networking \u00b6 My current cluster-internal networking is implemented by calico . Running high-available control-plane \u00b6 In order to expose my control-plane on a loadbalanced IP address, I have deployed kube-vip . It is configured to expose a load balanced address to the host IP addresses of my control-plane nodes over BGP. Exposing services on their own IP address \u00b6 Most (http/https) traffic enters my cluster through an Ingress controller. For situations where this is not desirable (e.g. MQTT traffic) or when I need a fixed IP reachable from outside the cluster (e.g. to use in combination with port forwarding) I use metallb in layer2 mode . Using this setup I can define a Service to use a loadBalancerIP , and it will be exposed on my network on that given IP address. Mixed-protocol services \u00b6 I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine UDP and TCP ports on the same Service. BGP \u00b6 Due to the way that BGP works, a node can only set up a single BGP connection to the router. Since I am already running kube-vip in BGP mode and I have a limited number of nodes, I am currently not using BGP mode to expose my services. Note Currently when using BGP on Opnsense, services do not get properly load balanced. This is due to Opnsense not supporting multipath in the BSD kernel","title":"Networking"},{"location":"networking/#networking","text":"My current cluster-internal networking is implemented by calico .","title":"Networking"},{"location":"networking/#running-high-available-control-plane","text":"In order to expose my control-plane on a loadbalanced IP address, I have deployed kube-vip . It is configured to expose a load balanced address to the host IP addresses of my control-plane nodes over BGP.","title":"Running high-available control-plane"},{"location":"networking/#exposing-services-on-their-own-ip-address","text":"Most (http/https) traffic enters my cluster through an Ingress controller. For situations where this is not desirable (e.g. MQTT traffic) or when I need a fixed IP reachable from outside the cluster (e.g. to use in combination with port forwarding) I use metallb in layer2 mode . Using this setup I can define a Service to use a loadBalancerIP , and it will be exposed on my network on that given IP address.","title":"Exposing services on their own IP address"},{"location":"networking/#mixed-protocol-services","text":"I have enabled the MixedProtocolLBService=true feature-gate on my cluster. This means that I can combine UDP and TCP ports on the same Service.","title":"Mixed-protocol services"},{"location":"networking/#bgp","text":"Due to the way that BGP works, a node can only set up a single BGP connection to the router. Since I am already running kube-vip in BGP mode and I have a limited number of nodes, I am currently not using BGP mode to expose my services. Note Currently when using BGP on Opnsense, services do not get properly load balanced. This is due to Opnsense not supporting multipath in the BSD kernel","title":"BGP"},{"location":"networking/dns/","text":"DNS \u00b6","title":"DNS"},{"location":"networking/dns/#dns","text":"","title":"DNS"},{"location":"networking/multus/","text":"Multus \u00b6","title":"Multus"},{"location":"networking/multus/#multus","text":"","title":"Multus"},{"location":"networking/podgateway/","text":"Pod Gateway \u00b6","title":"Pod Gateway"},{"location":"networking/podgateway/#pod-gateway","text":"","title":"Pod Gateway"},{"location":"storage/","text":"Storage \u00b6 Storage in my cluster is handled in a number of ways. The in-cluster storage is provided by a rook-ceph cluster that is running on a number of my nodes. rook-ceph block storage \u00b6 The bulk of my cluster storage relies on my CephBlockPool ( link ). This ensures that my data is replicated across my storage nodes. rook-ceph file storage \u00b6 A select few workloads require storage that is ReadWriteMany ( Kubernetes docs ). To keep this data in-cluster as well I have deployed a CephFilesystem ( link ), which is also replicated across my storage nodes. NFS storage \u00b6 Finally, I have my NAS that exposes several exports over NFS. Given how NFS is a very bad idea for storing application data (see for example this Github issue ) I only use it to store data at rest, such as my personal media files, Linux ISO's, backups, etc.","title":"Storage"},{"location":"storage/#storage","text":"Storage in my cluster is handled in a number of ways. The in-cluster storage is provided by a rook-ceph cluster that is running on a number of my nodes.","title":"Storage"},{"location":"storage/#rook-ceph-block-storage","text":"The bulk of my cluster storage relies on my CephBlockPool ( link ). This ensures that my data is replicated across my storage nodes.","title":"rook-ceph block storage"},{"location":"storage/#rook-ceph-file-storage","text":"A select few workloads require storage that is ReadWriteMany ( Kubernetes docs ). To keep this data in-cluster as well I have deployed a CephFilesystem ( link ), which is also replicated across my storage nodes.","title":"rook-ceph file storage"},{"location":"storage/#nfs-storage","text":"Finally, I have my NAS that exposes several exports over NFS. Given how NFS is a very bad idea for storing application data (see for example this Github issue ) I only use it to store data at rest, such as my personal media files, Linux ISO's, backups, etc.","title":"NFS storage"},{"location":"storage/backups/","text":"Backups \u00b6 Backups of the data that lives inside my cluster are handled by Kasten K10 . This is a commercial backup solution, but is free to use for up to 10 nodes. Please see their website to see if your use case falls under the license agreement. Kasten documentation \u00b6 Others have detailed this tool much better than I can, so I am going to be a bit lazy and just dump a few links here. How to configure Kasten K10 Disaster Recovery Kasten K10 documentation A hands-on lab that goes through the steps of backing up and restoring an application How I back up my data \u00b6 If you have gotten this far, you will now know that K10 introduces the concepts of Profiles and Policies. In summary, a Profile tells K10 where to store the data, and a Policy tells it what you want to back up. My current setup is that I have a single Profile ( link ) pointing to an NFS server. I then have a single Policy ( link ) that schedules snapshots and exports for: a set of namespaces all persistentVolumeClaim resources that have been assigned the label kasten.io/backup-volume: \"true\"","title":"Backups"},{"location":"storage/backups/#backups","text":"Backups of the data that lives inside my cluster are handled by Kasten K10 . This is a commercial backup solution, but is free to use for up to 10 nodes. Please see their website to see if your use case falls under the license agreement.","title":"Backups"},{"location":"storage/backups/#kasten-documentation","text":"Others have detailed this tool much better than I can, so I am going to be a bit lazy and just dump a few links here. How to configure Kasten K10 Disaster Recovery Kasten K10 documentation A hands-on lab that goes through the steps of backing up and restoring an application","title":"Kasten documentation"},{"location":"storage/backups/#how-i-back-up-my-data","text":"If you have gotten this far, you will now know that K10 introduces the concepts of Profiles and Policies. In summary, a Profile tells K10 where to store the data, and a Policy tells it what you want to back up. My current setup is that I have a single Profile ( link ) pointing to an NFS server. I then have a single Policy ( link ) that schedules snapshots and exports for: a set of namespaces all persistentVolumeClaim resources that have been assigned the label kasten.io/backup-volume: \"true\"","title":"How I back up my data"}]}