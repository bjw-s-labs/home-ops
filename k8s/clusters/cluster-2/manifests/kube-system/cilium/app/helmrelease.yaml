---
apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
  name: cilium
  namespace: kube-system
  # annotations:
  #   meta.helm.sh/release-name: cilium
  #   meta.helm.sh/release-namespace: kube-system
  # labels:
  #   app.kubernetes.io/managed-by: Helm
spec:
  interval: 5m
  chart:
    spec:
      chart: cilium
      version: 1.15.1
      sourceRef:
        kind: HelmRepository
        name: cilium-charts
        namespace: flux-system
      interval: 5m
  install:  
    remediation: # perform remediation when helm install fails
      retries: 100
  upgrade:
    remediation: # perform remediation when helm upgrade fails
      retries: 100
      remediateLastFailure: true # remediate the last failure, when no retries remain
    cleanupOnFail: true
  values:
    securityContext:
      capabilities:
        ciliumAgent:
          - CHOWN
          - KILL
          - NET_ADMIN
          - NET_RAW
          - IPC_LOCK
          - SYS_ADMIN
          - SYS_RESOURCE
          - DAC_OVERRIDE
          - FOWNER
          - SETGID
          - SETUID
        cleanCiliumState:
          - NET_ADMIN
          - SYS_ADMIN
          - SYS_RESOURCE
    cgroup:
      autoMount:
        enabled: false
      hostRoot: /sys/fs/cgroup

    cluster:
      name: "${CLUSTER_NAME}"
      id: "${CLUSTER_ID}"

    cni:
      exclusive: false   # Needed to allow Cillium to work properly with Multus

    rollOutCiliumPods: true
    localRedirectPolicy: true

    # Changed from "strict" to "true"
    kubeProxyReplacement: "true"
    kubeProxyReplacementHealthzBindAddr: 0.0.0.0:10256

    ipv4NativeRoutingCIDR: ${NETWORK_K8S_POD_CIDR}

    # k8sServiceHost: "cluster-2.${HARDWARE_DOMAIN}"
    # k8sServicePort: 6443
    # Changed to use KubePrism instead of the HAProxy instance. 
    # Should allow for cluster comms even when HAProxy goes down
    k8sServiceHost: localhost
    k8sServicePort: 7445

    loadBalancer:
      algorithm: "maglev"
      mode: "dsr"

    ipam:
      mode: "kubernetes"

    ingressController:
      enabled: true
      loadbalancerMode: shared

    service:
      loadBalancerIP: "${LB_INGRESS}"

    # tunnel: "disabled"
    tunnelProtocol: ""   #Disables tunnel
    routingMode: "native" 

    autoDirectNodeRoutes: true

    # endpointRoutes:
    #   enabled: true

    operator:
      rollOutPods: true

    containerRuntime:
      integration: containerd

    hubble:
      enabled: true
      serviceMonitor:
        enabled: true
      dashboards:
        enabled: true
        label: grafana_dashboard
        namespace: system-monitoring
        labelValue: "1"
        annotations: { }
      metrics:
        enabled:
          - dns:query;ignoreAAAA
          - drop
          - tcp
          - flow
          - port-distribution
          - icmp
          - http
      relay:
        enabled: true
        rollOutPods: true
        affinity:
          # Added affinity
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/rook-osd-node
                  operator: DoesNotExist
          # Default affinity in values.yaml
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  k8s-app: cilium
      ui:
        enabled: true
        rollOutPods: true
        ingress:
          enabled: true
          className: "internal-nginx"
          hosts:
            - &host "hubble.${CLUSTER_NAME}.${INGRESS_DOMAIN}"
          tls:
            - hosts:
                - *host

    bgpControlPlane:
      enabled: true
    bgp:
      enabled: false
      announce:
        loadbalancerIP: true
        podCIDR: true
